{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9014fe2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m     sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, src_path)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 18\u001b[0m   sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(\u001b[38;5;18;43m__file__\u001b[39;49m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrc\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mconfig\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_tf_dataset, plot_sample_images\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    src_path = '/content/Multimodal-Vehicle-Damage-Assessor/src'\n",
    "    os.chdir('/content/Multimodal-Vehicle-Damage-Assessor')\n",
    "    sys.path.insert(0, src_path)\n",
    "else:\n",
    "  sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))\n",
    "\n",
    "import config\n",
    "from data_loader import create_tf_dataset, plot_sample_images\n",
    "from model_builder import build_model, unfreeze_model\n",
    "from train_utils import (\n",
    "    compile_model, get_callbacks, train_model, \n",
    "    evaluate_model, get_predictions, calculate_metrics,\n",
    "    save_training_results, compare_models as compare_model_metrics\n",
    ")\n",
    "from visualize import (\n",
    "    plot_training_history, plot_confusion_matrix,\n",
    "    plot_classification_report, plot_roc_curves,\n",
    "    compare_models, plot_model_comparison_table,\n",
    "    plot_per_class_performance, create_comprehensive_report\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78514165",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Exploration\n",
    "\n",
    "Load the dataset and visualize sample images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d19f5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count images in each class\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for split in ['training', 'validation']:\n",
    "    print(f\"\\n{split.upper()}:\")\n",
    "    split_dir = os.path.join(config.SEVERITY_DATA_DIR, split)\n",
    "    \n",
    "    for class_name in config.SEVERITY_CLASS_NAMES:\n",
    "        class_folder = [f for f in os.listdir(split_dir) if class_name.lower() in f.lower()]\n",
    "        if class_folder:\n",
    "            class_path = os.path.join(split_dir, class_folder[0])\n",
    "            count = len(os.listdir(class_path))\n",
    "            print(f\"  {class_name}: {count} images\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a973c846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset for visualization (using EfficientNetB4 preprocessing)\n",
    "print(\"\\nLoading dataset for visualization...\")\n",
    "sample_train_ds, sample_val_ds, sample_info = create_tf_dataset('efficientnet_b4')\n",
    "\n",
    "print(\"\\nDataset Info:\")\n",
    "for key, value in sample_info.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e257747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images\n",
    "fig = plot_sample_images(sample_train_ds, num_images=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63cdbe4",
   "metadata": {},
   "source": [
    "## 3. Model Training\n",
    "\n",
    "Train all models and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cddcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store all results\n",
    "all_results = {}\n",
    "all_histories = {}\n",
    "all_checkpoints = {}\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(config.RANDOM_SEED)\n",
    "tf.random.set_seed(config.RANDOM_SEED)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STARTING MODEL TRAINING\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f9e7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train each model\n",
    "for model_name in config.MODELS_TO_TRAIN:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"TRAINING: {model_name.upper()}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Create dataset with model-specific preprocessing\n",
    "        print(f\"Step 1: Creating dataset for {model_name}...\")\n",
    "        train_ds, val_ds, ds_info = create_tf_dataset(model_name)\n",
    "        \n",
    "        # 2. Build model\n",
    "        print(f\"\\nStep 2: Building {model_name} model...\")\n",
    "        model, base_model = build_model(model_name, num_classes=config.SEVERITY_NUM_CLASSES)\n",
    "        \n",
    "        # 3. Compile model\n",
    "        print(f\"\\nStep 3: Compiling model...\")\n",
    "        model = compile_model(model, learning_rate=config.INITIAL_LEARNING_RATE)\n",
    "        \n",
    "        # 4. Get callbacks\n",
    "        print(f\"\\nStep 4: Setting up callbacks...\")\n",
    "        callbacks, checkpoint_path = get_callbacks(model_name, stage='initial')\n",
    "        \n",
    "        # 5. Train model\n",
    "        print(f\"\\nStep 5: Training model...\")\n",
    "        start_time = datetime.now()\n",
    "        history, checkpoint_path = train_model(\n",
    "            model, train_ds, val_ds, \n",
    "            model_name=model_name,\n",
    "            epochs=config.EPOCHS,\n",
    "            callbacks_list=callbacks\n",
    "        )\n",
    "        training_time = (datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "        # 6. Evaluate model\n",
    "        print(f\"\\nStep 6: Evaluating model...\")\n",
    "        eval_results = evaluate_model(model, val_ds, model_name)\n",
    "        \n",
    "        # 7. Get predictions\n",
    "        print(f\"\\nStep 7: Generating predictions...\")\n",
    "        y_true, y_pred, y_pred_proba = get_predictions(model, val_ds)\n",
    "        \n",
    "        # 8. Calculate detailed metrics\n",
    "        print(f\"\\nStep 8: Calculating metrics...\")\n",
    "        detailed_metrics = calculate_metrics(y_true, y_pred, y_pred_proba)\n",
    "        \n",
    "        # 9. Combine all results\n",
    "        combined_results = {\n",
    "            **eval_results,\n",
    "            **detailed_metrics,\n",
    "            'training_time': training_time,\n",
    "            'checkpoint_path': checkpoint_path\n",
    "        }\n",
    "        \n",
    "        # 10. Save results\n",
    "        print(f\"\\nStep 9: Saving results...\")\n",
    "        results_file = save_training_results(\n",
    "            model_name, history, combined_results, checkpoint_path\n",
    "        )\n",
    "        \n",
    "        # Store for comparison\n",
    "        all_results[model_name] = combined_results\n",
    "        all_histories[model_name] = history.history\n",
    "        all_checkpoints[model_name] = checkpoint_path\n",
    "        \n",
    "        # 11. Plot training history\n",
    "        print(f\"\\nStep 10: Plotting training history...\")\n",
    "        plot_training_history(history.history, model_name, save_dir=config.RESULTS_DIR)\n",
    "        plt.show()\n",
    "        \n",
    "        # 12. Plot confusion matrix\n",
    "        print(f\"\\nStep 11: Plotting confusion matrix...\")\n",
    "        plot_confusion_matrix(y_true, y_pred, model_name, save_dir=config.RESULTS_DIR)\n",
    "        plt.show()\n",
    "        \n",
    "        # 13. Classification report\n",
    "        print(f\"\\nStep 12: Generating classification report...\")\n",
    "        plot_classification_report(y_true, y_pred, model_name, save_dir=config.RESULTS_DIR)\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"‚úì {model_name.upper()} TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "        print(f\"  - Training Time: {training_time:.2f} seconds\")\n",
    "        print(f\"  - Val Accuracy: {eval_results['accuracy']:.4f}\")\n",
    "        print(f\"  - Val Loss: {eval_results['loss']:.4f}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        # Clear memory\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"‚úó ERROR TRAINING {model_name.upper()}: {str(e)}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        continue\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL MODELS TRAINING COMPLETED!\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d05ca75",
   "metadata": {},
   "source": [
    "## 4. Model Comparison\n",
    "\n",
    "Compare all models and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ee5154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comparison table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "comparison_df = compare_model_metrics(all_results)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811551b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig = compare_models(all_results, save_dir=config.RESULTS_DIR)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293d6038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table visualization\n",
    "comparison_df, fig = plot_model_comparison_table(all_results, save_dir=config.RESULTS_DIR)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93c75c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot per-class performance\n",
    "fig = plot_per_class_performance(all_results, save_dir=config.RESULTS_DIR)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e58608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive report\n",
    "create_comprehensive_report(all_results, save_dir=config.RESULTS_DIR)\n",
    "print(\"\\n‚úì Comprehensive report generated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430c2894",
   "metadata": {},
   "source": [
    "## 5. Best Model Selection\n",
    "\n",
    "Identify and highlight the best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f562da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best model\n",
    "best_model_name = max(all_results.items(), key=lambda x: x[1]['accuracy'])[0]\n",
    "best_accuracy = all_results[best_model_name]['accuracy']\n",
    "best_checkpoint = all_checkpoints[best_model_name]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BEST MODEL SELECTION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name.upper()}\")\n",
    "print(f\"   Accuracy: {best_accuracy:.4f}\")\n",
    "print(f\"   Precision: {all_results[best_model_name]['precision']:.4f}\")\n",
    "print(f\"   Recall: {all_results[best_model_name]['recall']:.4f}\")\n",
    "print(f\"   F1-Score: {all_results[best_model_name]['f1_score']:.4f}\")\n",
    "print(f\"   Training Time: {all_results[best_model_name]['training_time']:.2f} seconds\")\n",
    "print(f\"\\n   Saved at: {best_checkpoint}\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1925fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display per-class metrics for best model\n",
    "print(f\"\\nPer-Class Performance for {best_model_name.upper()}:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for class_name in config.SEVERITY_CLASS_NAMES:\n",
    "    if class_name in all_results[best_model_name]['per_class_metrics']:\n",
    "        metrics = all_results[best_model_name]['per_class_metrics'][class_name]\n",
    "        print(f\"\\n{class_name.upper()}:\")\n",
    "        print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"  Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"  F1-Score: {metrics['f1_score']:.4f}\")\n",
    "        print(f\"  Support: {metrics['support']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874752fe",
   "metadata": {},
   "source": [
    "## 6. Summary & Recommendations\n",
    "\n",
    "Based on the training results, here are the key findings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd960c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"Models Trained:\", len(all_results))\n",
    "print(\"\\nAccuracy Range:\")\n",
    "accuracies = [results['accuracy'] for results in all_results.values()]\n",
    "print(f\"  Best: {max(accuracies):.4f}\")\n",
    "print(f\"  Worst: {min(accuracies):.4f}\")\n",
    "print(f\"  Average: {np.mean(accuracies):.4f}\")\n",
    "print(f\"  Std Dev: {np.std(accuracies):.4f}\")\n",
    "\n",
    "print(\"\\nTraining Time:\")\n",
    "times = [results['training_time'] for results in all_results.values()]\n",
    "print(f\"  Fastest: {min(times):.2f} seconds\")\n",
    "print(f\"  Slowest: {max(times):.2f} seconds\")\n",
    "print(f\"  Average: {np.mean(times):.2f} seconds\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96eff722",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Fine-tuning**: Unfreeze some layers of the best model for additional training\n",
    "2. **Hyperparameter Optimization**: Try different learning rates, batch sizes, etc.\n",
    "3. **Ensemble Methods**: Combine predictions from multiple models\n",
    "4. **Deploy**: Save the best model for production use\n",
    "5. **Test on New Data**: Validate on unseen vehicle damage images\n",
    "\n",
    "## Recommendations\n",
    "\n",
    "- **For Production**: Use the best accuracy model if accuracy is critical\n",
    "- **For Mobile/Edge**: Use MobileNetV2 for resource-constrained environments\n",
    "- **For Balance**: Use EfficientNetB4 for best accuracy/speed tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eae14a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary to file\n",
    "summary_path = os.path.join(config.RESULTS_DIR, 'training_summary.txt')\n",
    "\n",
    "with open(summary_path, 'w') as f:\n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "    f.write(\"VEHICLE DAMAGE SEVERITY CLASSIFICATION - TRAINING SUMMARY\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(f\"Training Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(f\"Models Trained: {len(all_results)}\\n\\n\")\n",
    "    \n",
    "    f.write(\"BEST MODEL\\n\")\n",
    "    f.write(\"-\" * 40 + \"\\n\")\n",
    "    f.write(f\"Model: {best_model_name}\\n\")\n",
    "    f.write(f\"Accuracy: {best_accuracy:.4f}\\n\")\n",
    "    f.write(f\"Checkpoint: {best_checkpoint}\\n\\n\")\n",
    "    \n",
    "    f.write(\"ALL MODELS PERFORMANCE\\n\")\n",
    "    f.write(\"-\" * 40 + \"\\n\")\n",
    "    f.write(comparison_df.to_string(index=False))\n",
    "    f.write(\"\\n\\n\")\n",
    "    \n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(f\"\\n‚úì Training summary saved to: {summary_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
